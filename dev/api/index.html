<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · MLJLinearModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJLinearModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick start</a></li><li><a class="tocitem" href="../models/">Models</a></li><li><a class="tocitem" href="../solvers/">Solvers</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Standalone-1"><span>Standalone</span></a></li><li><a class="tocitem" href="#MLJ-Interface-1"><span>MLJ Interface</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h1><h2 id="Standalone-1"><a class="docs-heading-anchor" href="#Standalone-1">Standalone</a><a class="docs-heading-anchor-permalink" href="#Standalone-1" title="Permalink"></a></h2><h3 id="Regression-1"><a class="docs-heading-anchor" href="#Regression-1">Regression</a><a class="docs-heading-anchor-permalink" href="#Regression-1" title="Permalink"></a></h3><p><strong>Standard constructors</strong></p><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LinearRegression" href="#MLJLinearModels.LinearRegression"><code>MLJLinearModels.LinearRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LinearRegression()
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RidgeRegression" href="#MLJLinearModels.RidgeRegression"><code>MLJLinearModels.RidgeRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">RidgeRegression()
RidgeRegression(λ)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LassoRegression" href="#MLJLinearModels.LassoRegression"><code>MLJLinearModels.LassoRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LassoRegression()
LassoRegression(λ)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₁$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₁$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.ElasticNetRegression" href="#MLJLinearModels.ElasticNetRegression"><code>MLJLinearModels.ElasticNetRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ElasticNetRegression()
ElasticNetRegression(λ)
ElasticNetRegression(λ, γ)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.HuberRegression" href="#MLJLinearModels.HuberRegression"><code>MLJLinearModels.HuberRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">HuberRegression()
HuberRegression(δ)
HuberRegression(δ, λ)
HuberRegression(δ, λ, γ)
</code></pre><p>Huber Regression with objective:</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>Where <code>ρ</code> is the Huber function <code>ρ(r) = r²/2</code><code>if</code>|r|≤δ<code>and</code>ρ(r)=δ(|r|-δ/2)<code>otherwise and</code><code>n</code><code>is the number of samples</code>size(X, 1)<code>. With</code>scale<em>penalty</em>with_samples = false<code>the objective function is</code><code>∑ρ(Xθ - y) + λ|θ|₂²/2 + γ|θ|₁</code>`.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.QuantileRegression" href="#MLJLinearModels.QuantileRegression"><code>MLJLinearModels.QuantileRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">QuantileRegression()
QuantileRegression(δ)
QuantileRegression(δ, λ)
QuantileRegression(δ, λ, γ)
</code></pre><p>Quantile Regression with objective:</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>Where <code>ρ</code> is the check function <code>ρ(r) = r(δ - 1(r &lt; 0))</code> and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L239">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LADRegression" href="#MLJLinearModels.LADRegression"><code>MLJLinearModels.LADRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LADRegression()
LADRegression(λ)
LADRegression(λ, γ)
</code></pre><p>Least Absolute Deviation regression with objective:</p><p><span>$|Xθ - y|₁ + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₁ + λ|θ|₂²/2 + γ|θ|₁$</span>.</p><p>This is a specific type of Quantile Regression with <code>δ=0.5</code> (median).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L263">source</a></section></article><p><strong>Generic constructors</strong></p><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.GeneralizedLinearRegression" href="#MLJLinearModels.GeneralizedLinearRegression"><code>MLJLinearModels.GeneralizedLinearRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GeneralizedLinearRegression{L&lt;:Loss, P&lt;:Penalty}</code></pre><p>Generalized Linear Regression (GLR) model with objective function:</p><p><span>$L(y, Xθ) + P(θ)$</span></p><p>where <code>L</code> is a loss function, <code>P</code> a penalty, <code>y</code> is the vector of observed response, <code>X</code> is the feature matrix and <code>θ</code> the vector of parameters. If <code>scale_penalty_with_samples = true</code> (default) the penalty is automatically scaled with the number of samples.</p><p>Special cases include:</p><ul><li><strong>OLS regression</strong>:      L2 loss, no penalty.</li><li><strong>Ridge regression</strong>:    L2 loss, L2 penalty.</li><li><strong>Lasso regression</strong>:    L2 loss, L1 penalty.</li><li><strong>Logistic regression</strong>: Logit loss, [no,L1,L2] penalty.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L8-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RobustRegression" href="#MLJLinearModels.RobustRegression"><code>MLJLinearModels.RobustRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">RobustRegression()
RobustRegression(ρ)
RobustRegression(ρ, λ)
RobustRegression(ρ, λ, γ)
</code></pre><p>Objective function: <span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span> where ρ is a given function on the residuals and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L191">source</a></section></article><h3 id="Classification-1"><a class="docs-heading-anchor" href="#Classification-1">Classification</a><a class="docs-heading-anchor-permalink" href="#Classification-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LogisticRegression" href="#MLJLinearModels.LogisticRegression"><code>MLJLinearModels.LogisticRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LogisticRegression()
LogisticRegression(λ)
LogisticRegression(λ, γ)
</code></pre><p>Objective function: <span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <code>L</code> is either the logistic loss in the binary case or the multinomial loss otherwise and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.MultinomialRegression" href="#MLJLinearModels.MultinomialRegression"><code>MLJLinearModels.MultinomialRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MultinomialRegression(a)
</code></pre><p>Objective function: <span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <code>L</code> is the multinomial loss and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/glr/constructors.jl#L176">source</a></section></article><h2 id="MLJ-Interface-1"><a class="docs-heading-anchor" href="#MLJ-Interface-1">MLJ Interface</a><a class="docs-heading-anchor-permalink" href="#MLJ-Interface-1" title="Permalink"></a></h2><h3 id="Regressors-1"><a class="docs-heading-anchor" href="#Regressors-1">Regressors</a><a class="docs-heading-anchor-permalink" href="#Regressors-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LinearRegressor" href="#MLJLinearModels.LinearRegressor"><code>MLJLinearModels.LinearRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LinearRegressor</code></pre><p>A model type for constructing a linear regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">LinearRegressor = @load LinearRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = LinearRegressor()</code> to construct an instance with default hyper-parameters.</p><p>This model provides standard linear regression with objective function</p><p><span>$|Xθ - y|₂²/2$</span></p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>&quot;any instance of <code>MLJLinearModels.Analytical</code>. Use <code>Analytical()</code> for Cholesky and <code>CG()=Analytical(iterative=true)</code> for conjugate-gradient.</p><p>If <code>solver = nothing</code> (default) then <code>Analytical()</code> is used.  Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LinearRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L5-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RidgeRegressor" href="#MLJLinearModels.RidgeRegressor"><code>MLJLinearModels.RidgeRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RidgeRegressor</code></pre><p>A model type for constructing a ridge regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">RidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = RidgeRegressor()</code> to construct an instance with default hyper-parameters.</p><p>Ridge regression is a linear model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2$</span></p><p>where <span>$n$</span> is the number of observations.</p><p>If <code>scale_penalty_with_samples = false</code> then the objective function is instead</p><p><span>$|Xθ - y|₂²/2 + λ|θ|₂²/2$</span>.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L2 regularization. Default: 1.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>any instance of <code>MLJLinearModels.Analytical</code>. Use <code>Analytical()</code> for Cholesky and <code>CG()=Analytical(iterative=true)</code> for conjugate-gradient. If <code>solver = nothing</code> (default) then <code>Analytical()</code> is used.  Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(RidgeRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.ElasticNetRegressor"><code>ElasticNetRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L55-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LassoRegressor" href="#MLJLinearModels.LassoRegressor"><code>MLJLinearModels.LassoRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LassoRegressor</code></pre><p>A model type for constructing a lasso regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">LassoRegressor = @load LassoRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = LassoRegressor()</code> to construct an instance with default hyper-parameters.</p><p>Lasso regression is a linear model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₁$</span></p><p>where <span>$n$</span> is the number of observations.</p><p>If <code>scale_penalty_with_samples = false</code> the objective function is</p><p><span>$|Xθ - y|₂²/2 + λ|θ|₁$</span>.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L1 regularization. Default: 1.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>any instance of <code>MLJLinearModels.ProxGrad</code>. If <code>solver=nothing</code> (default) then <code>ProxGrad(accel=true)</code> (FISTA) is used. Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code>.  Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LassoRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.ElasticNetRegressor"><code>ElasticNetRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L122-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.ElasticNetRegressor" href="#MLJLinearModels.ElasticNetRegressor"><code>MLJLinearModels.ElasticNetRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ElasticNetRegressor</code></pre><p>A model type for constructing a elastic net regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">ElasticNetRegressor = @load ElasticNetRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = ElasticNetRegressor()</code> to construct an instance with default hyper-parameters.</p><p>Elastic net is a linear model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>where <span>$n$</span> is the number of observations.</p><p>If  <code>scale_penalty_with_samples = false</code> the objective function is instead</p><p><span>$|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁$</span>.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L2 regularization. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularization. Default: 0.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>any instance of <code>MLJLinearModels.ProxGrad</code>.</p><p>If <code>solver=nothing</code> (default) then <code>ProxGrad(accel=true)</code> (FISTA) is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code>.  Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(ElasticNetRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.LassoRegressor"><code>LassoRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L191-L213">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.HuberRegressor" href="#MLJLinearModels.HuberRegressor"><code>MLJLinearModels.HuberRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HuberRegressor</code></pre><p>A model type for constructing a huber regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">HuberRegressor = @load HuberRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = HuberRegressor()</code> to construct an instance with default hyper-parameters.</p><p>This model coincides with <a href="#MLJLinearModels.RobustRegressor"><code>RobustRegressor</code></a>, with the exception that the robust loss, <code>rho</code>, is fixed to <code>HuberRho(delta)</code>, where <code>delta</code> is a new hyperparameter.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>delta::Real</code></p><p>parameterizes the <code>HuberRho</code> function (radius of the ball within which the loss     is a quadratic loss) Default: 0.5</p></li><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regularizer if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>IWLSCG</code>, <code>Newton</code>, <code>NewtonCG</code>, if <code>penalty = :l2</code>, and <code>ProxGrad</code> otherwise.</p><p>If <code>solver = nothing</code> (default) then <code>LBFGS()</code> is used, if <code>penalty = :l2</code>, and otherwise <code>ProxGrad(accel=true)</code> (FISTA) is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(HuberRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.RobustRegressor"><code>RobustRegressor</code></a>, <a href="#MLJLinearModels.QuantileRegressor"><code>QuantileRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L348-L370">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.QuantileRegressor" href="#MLJLinearModels.QuantileRegressor"><code>MLJLinearModels.QuantileRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">QuantileRegressor</code></pre><p>A model type for constructing a quantile regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">QuantileRegressor = @load QuantileRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = QuantileRegressor()</code> to construct an instance with default hyper-parameters.</p><p>This model coincides with <a href="#MLJLinearModels.RobustRegressor"><code>RobustRegressor</code></a>, with the exception that the robust loss, <code>rho</code>, is fixed to <code>QuantileRho(delta)</code>, where <code>delta</code> is a new hyperparameter.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>delta::Real</code></p><p>parameterizes the <code>QuantileRho</code> function (indicating the quantile to use     with default <code>0.5</code> for the median regression) Default: 0.5</p></li><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regularizer if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>IWLSCG</code>, if <code>penalty = :l2</code>, and <code>ProxGrad</code> otherwise.</p><p>If <code>solver = nothing</code> (default) then <code>LBFGS()</code> is used, if <code>penalty = :l2</code>, and otherwise <code>ProxGrad(accel=true)</code> (FISTA) is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(QuantileRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.RobustRegressor"><code>RobustRegressor</code></a>, <a href="#MLJLinearModels.HuberRegressor"><code>HuberRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L422-L444">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LADRegressor" href="#MLJLinearModels.LADRegressor"><code>MLJLinearModels.LADRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LADRegressor</code></pre><p>A model type for constructing a lad regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">LADRegressor = @load LADRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = LADRegressor()</code> to construct an instance with default hyper-parameters.</p><p>Least absolute deviation regression is a linear model with objective function</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span></p><p>where <span>$ρ$</span> is the absolute loss and <span>$n$</span> is the number of observations.</p><p>If <code>scale_penalty_with_samples = false</code> the objective function is instead</p><p><span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span>.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><p>See also <code>RobustRegressor</code>.</p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regularizer if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>IWLSCG</code>, if <code>penalty = :l2</code>, and <code>ProxGrad</code> otherwise.</p><p>If <code>solver = nothing</code> (default) then <code>LBFGS()</code> is used, if <code>penalty = :l2</code>, and otherwise <code>ProxGrad(accel=true)</code> (FISTA) is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LADRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L496-L518">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RobustRegressor" href="#MLJLinearModels.RobustRegressor"><code>MLJLinearModels.RobustRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RobustRegressor</code></pre><p>A model type for constructing a robust regressor, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">RobustRegressor = @load RobustRegressor pkg=MLJLinearModels</code></pre><p>Do <code>model = RobustRegressor()</code> to construct an instance with default hyper-parameters.</p><p>Robust regression is a linear model with objective function</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span></p><p>where <span>$ρ$</span> is a robust loss function (e.g. the Huber function) and <span>$n$</span> is the number of observations.</p><p>If <code>scale_penalty_with_samples = false</code> the objective function is instead</p><p><span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span>.</p><p>Different solver options exist, as indicated under &quot;Hyperparameters&quot; below. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>rho::MLJLinearModels.RobustRho</code></p><p>the type of robust loss, which can be any instance of     <code>MLJLinearModels.L</code> where <code>L</code> is one of: <code>AndrewsRho</code>,     <code>BisquareRho</code>, <code>FairRho</code>, <code>HuberRho</code>, <code>LogisticRho</code>,     <code>QuantileRho</code>, <code>TalwarRho</code>, <code>HuberRho</code>, <code>TalwarRho</code>.  Default: HuberRho(0.1)</p></li><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regularizer if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of observations. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>IWLSCG</code>, <code>Newton</code>, <code>NewtonCG</code>, if <code>penalty = :l2</code>, and <code>ProxGrad</code> otherwise.</p><p>If <code>solver = nothing</code> (default) then <code>LBFGS()</code> is used, if <code>penalty = :l2</code>, and otherwise <code>ProxGrad(accel=true)</code> (FISTA) is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(RobustRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.HuberRegressor"><code>HuberRegressor</code></a>, <a href="#MLJLinearModels.QuantileRegressor"><code>QuantileRegressor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/regressors.jl#L264-L286">source</a></section></article><h3 id="Classifiers-1"><a class="docs-heading-anchor" href="#Classifiers-1">Classifiers</a><a class="docs-heading-anchor-permalink" href="#Classifiers-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LogisticClassifier" href="#MLJLinearModels.LogisticClassifier"><code>MLJLinearModels.LogisticClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LogisticClassifier</code></pre><p>A model type for constructing a logistic classifier, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels</code></pre><p>Do <code>model = LogisticClassifier()</code> to construct an instance with default hyper-parameters.</p><p>This model is more commonly known as &quot;logistic regression&quot;. It is a standard classifier for both binary and multiclass classification.  The objective function applies either a logistic loss (binary target) or multinomial (softmax) loss, and has a mixed L1/L2 penalty:</p><p><span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span>.</p><p>Here <span>$L$</span> is either <code>MLJLinearModels.LogisticLoss</code> or <code>MLJLinearModels.MultiClassLoss</code>, <span>$λ$</span> and <span>$γ$</span> indicate the strength of the L2 (resp. L1) regularization components and <span>$n$</span> is the number of training observations.</p><p>With <code>scale_penalty_with_samples = false</code> the objective function is instead</p><p><span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code> and strength of the L2     regularizer if <code>penalty</code> is <code>:en</code>. Default: eps()</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>Newton</code>, <code>NewtonCG</code>, <code>ProxGrad</code>; but subject to the following restrictions:</p><ul><li><p>If <code>penalty = :l2</code>, <code>ProxGrad</code> is disallowed. Otherwise, <code>ProxGrad</code> is the only option.</p></li><li><p>Unless <code>scitype(y) &lt;: Finite{2}</code> (binary target) <code>Newton</code> is disallowed.</p></li></ul><p>If <code>solver = nothing</code> (default) then <code>ProxGrad(accel=true)</code> (FISTA) is used, unless <code>gamma = 0</code>, in which case <code>LBFGS()</code> is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_blobs(centers = 2)
mach = fit!(machine(LogisticClassifier(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.MultinomialClassifier"><code>MultinomialClassifier</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/classifiers.jl#L5-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.MultinomialClassifier" href="#MLJLinearModels.MultinomialClassifier"><code>MLJLinearModels.MultinomialClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultinomialClassifier</code></pre><p>A model type for constructing a multinomial classifier, based on <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl">MLJLinearModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">MultinomialClassifier = @load MultinomialClassifier pkg=MLJLinearModels</code></pre><p>Do <code>model = MultinomialClassifier()</code> to construct an instance with default hyper-parameters.</p><p>This model coincides with <a href="#MLJLinearModels.LogisticClassifier"><code>LogisticClassifier</code></a>, except certain optimizations possible in the special binary case will not be applied. Its hyperparameters are identical.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>where:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code> scitype; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyperparameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regularizer if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regularizer if <code>penalty</code> is <code>:en</code>. Default: eps()</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularizer if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>some instance of <code>MLJLinearModels.S</code> where <code>S</code> is one of: <code>LBFGS</code>, <code>NewtonCG</code>, <code>ProxGrad</code>; but subject to the following restrictions:</p><ul><li><p>If <code>penalty = :l2</code>, <code>ProxGrad</code> is disallowed. Otherwise, <code>ProxGrad</code> is the only option.</p></li><li><p>Unless <code>scitype(y) &lt;: Finite{2}</code> (binary target) <code>Newton</code> is disallowed.</p></li></ul><p>If <code>solver = nothing</code> (default) then <code>ProxGrad(accel=true)</code> (FISTA) is used, unless <code>gamma = 0</code>, in which case <code>LBFGS()</code> is used.</p><p>Solver aliases: <code>FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)</code>, <code>ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)</code> Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_blobs(centers = 3)
mach = fit!(machine(MultinomialClassifier(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre><p>See also <a href="#MLJLinearModels.LogisticClassifier"><code>LogisticClassifier</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/27b377257a3de81b899f9fc616c36046fc3942be/src/mlj/classifiers.jl#L94-L116">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../solvers/">« Solvers</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 2 October 2023 23:39">Monday 2 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
