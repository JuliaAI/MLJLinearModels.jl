<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · MLJLinearModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJLinearModels.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Goals-for-the-package-1"><span>Goals for the package</span></a></li><li><a class="tocitem" href="#Quick-start-1"><span>Quick start</span></a></li><li><a class="tocitem" href="#Available-models-1"><span>Available models</span></a></li><li><a class="tocitem" href="#Limitations-1"><span>Limitations</span></a></li></ul></li><li><a class="tocitem" href="mlj/">MLJ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJLinearModels.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="MLJLinearModels.jl-1"><a class="docs-heading-anchor" href="#MLJLinearModels.jl-1">MLJLinearModels.jl</a><a class="docs-heading-anchor-permalink" href="#MLJLinearModels.jl-1" title="Permalink"></a></h1><p>This is a convenience package gathering functionalities to solve a number of generalised linear regression/classification problems which, inherently, correspond to an optimisation problem of the form</p><div>\[L(y, X\theta) + P(\theta)\]</div><p>where <span>$L$</span> is a <em>loss function</em> and <span>$P$</span> is a  <em>penalty function</em> (both of those can be scaled or composed).</p><p>A well known example is the <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Ridge regression</a> where the problem amounts to minimising</p><div>\[\|y - X\theta\|_2^2 + \lambda\|\theta\|_2^2.\]</div><h2 id="Goals-for-the-package-1"><a class="docs-heading-anchor" href="#Goals-for-the-package-1">Goals for the package</a><a class="docs-heading-anchor-permalink" href="#Goals-for-the-package-1" title="Permalink"></a></h2><ul><li>make these regressions models &quot;easy to call&quot; and callable in a unified way,</li><li>interface with <a href="https://github.com/alan-turing-institute/MLJ.jl"><code>MLJ.jl</code></a>,</li><li>focus on performance including in &quot;big data&quot; settings exploiting packages such as <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim.jl</code></a>, and <a href="https://github.com/JuliaMath/IterativeSolvers.jl"><code>IterativeSolvers.jl</code></a>,</li><li>use a &quot;machine learning&quot; perspective, i.e.: focus primarily on prediction, hyper-parameters should be obtained via a data-driven procedure such as cross-validation.</li></ul><p>All models allow to fit an intercept and allow the penalty to be optionally applied on the intercept (not applied by default). All models attempt to be efficient in terms of memory allocation to avoid unnecessary copies of the data.</p><h2 id="Quick-start-1"><a class="docs-heading-anchor" href="#Quick-start-1">Quick start</a><a class="docs-heading-anchor-permalink" href="#Quick-start-1" title="Permalink"></a></h2><p>The package works by</p><ol><li>specifying the kind of model you want along with its hyper-parameters,</li><li>calling <code>fit</code> with that model and the data: <code>fit(model, X, y)</code>.</li></ol><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The convention is that the feature matrix has dimensions <span>$n \times p$</span> where <span>$n$</span> is the number of records (points) and <span>$p$</span> is the number of features (dimensions).</p></div></div><h3 id="Lasso-regression-1"><a class="docs-heading-anchor" href="#Lasso-regression-1">Lasso regression</a><a class="docs-heading-anchor-permalink" href="#Lasso-regression-1" title="Permalink"></a></h3><p>The lasso regression corresponds to a l2-loss function with a l1-penalty:</p><div>\[\theta_{\text{Lasso}} = \frac12\|y-X\theta\|_2^2 + \lambda\|\theta\|_1\]</div><p>which you can create as follows:</p><pre><code class="language-julia">λ = 0.7
lasso = LassoRegression(0.7)
fit(lasso, X, y)</code></pre><h3 id="(Multinomial)-logistic-classifier-1"><a class="docs-heading-anchor" href="#(Multinomial)-logistic-classifier-1">(Multinomial) logistic classifier</a><a class="docs-heading-anchor-permalink" href="#(Multinomial)-logistic-classifier-1" title="Permalink"></a></h3><p>In a classification context, the multinomial logistic regression returns a predicted score per class that can be interpreted as the likelihood of a point belonging to a class given the trained model. It&#39;s given by the multinomial loss plus an optional penalty (typically the l2 penalty).</p><p>Here&#39;s a way to do this:</p><pre><code class="language-julia">λ = 0.1
mlr = MultinomialRegression(λ) # you can also just use LogisticRegression
fit(mlr, X, y)</code></pre><p>In a <strong>binary</strong> context, <span>$y$</span> is expected to have values <span>$y_i \in \{\pm 1\}$</span> whereas in the <strong>multiclass</strong> context, <span>$y$</span> is expected to have values <span>$y_i \in {1, \dots, c}$</span> where <span>$c &gt; 2$</span> is the number of classes.</p><h2 id="Available-models-1"><a class="docs-heading-anchor" href="#Available-models-1">Available models</a><a class="docs-heading-anchor-permalink" href="#Available-models-1" title="Permalink"></a></h2><h3 id="Regression-models-(continuous-target)-1"><a class="docs-heading-anchor" href="#Regression-models-(continuous-target)-1">Regression models (continuous target)</a><a class="docs-heading-anchor-permalink" href="#Regression-models-(continuous-target)-1" title="Permalink"></a></h3><table><tr><th style="text-align: left">Regressors</th><th style="text-align: left">Formulation¹</th><th style="text-align: left">Available solvers</th><th style="text-align: left">Comments</th></tr><tr><td style="text-align: left">OLS &amp; Ridge</td><td style="text-align: left">L2Loss + 0/L2</td><td style="text-align: left">Analytical² or CG³</td><td style="text-align: left"></td></tr><tr><td style="text-align: left">Lasso &amp; Elastic-Net</td><td style="text-align: left">L2Loss + 0/L2 + L1</td><td style="text-align: left">(F)ISTA⁴</td><td style="text-align: left"></td></tr><tr><td style="text-align: left">Robust 0/L2</td><td style="text-align: left">RobustLoss⁵ + 0/L2</td><td style="text-align: left">Newton, NewtonCG, LBFGS, IWLS-CG⁶</td><td style="text-align: left">no scale⁷</td></tr><tr><td style="text-align: left">Robust L1/EN</td><td style="text-align: left">RobustLoss + 0/L2 + L1</td><td style="text-align: left">(F)ISTA</td><td style="text-align: left"></td></tr><tr><td style="text-align: left">Quantile⁸ + 0/L2</td><td style="text-align: left">RobustLoss + 0/L2</td><td style="text-align: left">LBFGS, IWLS-CG</td><td style="text-align: left"></td></tr><tr><td style="text-align: left">Quantile L1/EN</td><td style="text-align: left">RobustLoss + 0/L2 + L1</td><td style="text-align: left">(F)ISTA</td><td style="text-align: left"></td></tr></table><ol><li>&quot;0&quot; stands for no penalty</li><li>Analytical means the solution is computed in &quot;one shot&quot; using the <code>\</code> solver,</li><li>CG = conjugate gradient</li><li>(Accelerated) Proximal Gradient Descent</li><li><em>Huber</em>, <em>Andrews</em>, <em>Bisquare</em>, <em>Logistic</em>, <em>Fair</em> and <em>Talwar</em> weighing functions available.</li><li>Iteratively re-Weighted Least Squares where each system is solved iteratively via CG</li><li>In other packages such as Scikit-Learn, a scale factor is estimated along with the parameters, this is a bit ad-hoc and corresponds more to a statistical perspective, further it does not work well with penalties; we recommend using cross-validation to set the parameter of the Huber Loss.</li><li>Includes as special case the <em>least absolute deviation</em> (LAD) regression when <code>δ=0.5</code>.</li></ol><h3 id="Classification-models-(finite-target)-1"><a class="docs-heading-anchor" href="#Classification-models-(finite-target)-1">Classification models (finite target)</a><a class="docs-heading-anchor-permalink" href="#Classification-models-(finite-target)-1" title="Permalink"></a></h3><table><tr><th style="text-align: left">Classifiers</th><th style="text-align: left">Formulation</th><th style="text-align: left">Available solvers</th><th style="text-align: left">Comments</th></tr><tr><td style="text-align: left">Logistic 0/L2</td><td style="text-align: left">LogisticLoss + 0/L2</td><td style="text-align: left">Newton, Newton-CG, LBFGS</td><td style="text-align: left"><code>yᵢ∈{±1}</code></td></tr><tr><td style="text-align: left">Logistic L1/EN</td><td style="text-align: left">LogisticLoss + 0/L2 + L1</td><td style="text-align: left">(F)ISTA</td><td style="text-align: left"><code>yᵢ∈{±1}</code></td></tr><tr><td style="text-align: left">Multinomial 0/L2</td><td style="text-align: left">MultinomialLoss + 0/L2</td><td style="text-align: left">Newton-CG, LBFGS</td><td style="text-align: left"><code>yᵢ∈{1,...,c}</code></td></tr><tr><td style="text-align: left">Multinomial L1/EN</td><td style="text-align: left">MultinomialLoss + 0/L2 + L1</td><td style="text-align: left">ISTA, FISTA</td><td style="text-align: left"><code>yᵢ∈{1,...,c}</code></td></tr></table><p>Unless otherwise specified:</p><ul><li>Newton-like solvers use Hager-Zhang line search (default in <a href="(https:/github.com/JuliaNLSolvers/Optim.jl)"><code>Optim.jl</code></a>)</li><li>ISTA, FISTA solvers use backtracking line search and a shrinkage factor of <code>β=0.8</code></li></ul><p><strong>Note</strong>: these models were all tested for correctness whenever a direct comparison with another package was possible, usually by comparing the objective function at the coefficients returned (cf. the tests):</p><ul><li>(<em>against <a href="https://scikit-learn.org/">scikit-learn</a></em>): Lasso, Elastic-Net, Logistic (L1/L2/EN), Multinomial (L1/L2/EN)</li><li>(<em>against <a href="https://cran.r-project.org/web/packages/quantreg/index.html">quantreg</a></em>): Quantile (0/L1)</li></ul><p>Systematic timing benchmarks have not been run yet but it&#39;s planned (see <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl/issues/14">this issue</a>).</p><h2 id="Limitations-1"><a class="docs-heading-anchor" href="#Limitations-1">Limitations</a><a class="docs-heading-anchor-permalink" href="#Limitations-1" title="Permalink"></a></h2><p>Note the current limitations:</p><ul><li>The models are built and tested assuming <code>n &gt; p</code>; if this doesn&#39;t hold, tricks should be employed to speed up computations; these have not been implemented yet.</li><li>CV-aware code not implemented yet (code that re-uses computations when fitting over a number of hyper-parameters);  &quot;Meta&quot; functionalities such as One-vs-All or Cross-Validation are left to other packages such as MLJ.</li><li>No support yet for sparse matrices.</li><li>Stochastic solvers have not yet been implemented.</li><li>All computations are assumed to be done in Float64.</li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="mlj/">MLJ »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 7 February 2020 18:37">Friday 7 February 2020</span>. Using Julia version 1.5.0-DEV.145.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
