<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · MLJLinearModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJLinearModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick start</a></li><li><a class="tocitem" href="../models/">Models</a></li><li><a class="tocitem" href="../solvers/">Solvers</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Standalone-1"><span>Standalone</span></a></li><li><a class="tocitem" href="#MLJ-Interface-1"><span>MLJ Interface</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h1><h2 id="Standalone-1"><a class="docs-heading-anchor" href="#Standalone-1">Standalone</a><a class="docs-heading-anchor-permalink" href="#Standalone-1" title="Permalink"></a></h2><h3 id="Regression-1"><a class="docs-heading-anchor" href="#Regression-1">Regression</a><a class="docs-heading-anchor-permalink" href="#Regression-1" title="Permalink"></a></h3><p><strong>Standard constructors</strong></p><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LinearRegression" href="#MLJLinearModels.LinearRegression"><code>MLJLinearModels.LinearRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LinearRegression(; fit_intercept)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RidgeRegression" href="#MLJLinearModels.RidgeRegression"><code>MLJLinearModels.RidgeRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">RidgeRegression()
RidgeRegression(λ; lambda, fit_intercept, penalize_intercept, scale_penalty_with_samples)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LassoRegression" href="#MLJLinearModels.LassoRegression"><code>MLJLinearModels.LassoRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LassoRegression()
LassoRegression(λ; lambda, fit_intercept, penalize_intercept, scale_penalty_with_samples)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₁$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₁$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.ElasticNetRegression" href="#MLJLinearModels.ElasticNetRegression"><code>MLJLinearModels.ElasticNetRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ElasticNetRegression()
ElasticNetRegression(λ)
ElasticNetRegression(λ, γ; lambda, gamma, fit_intercept, penalize_intercept, scale_penalty_with_samples)
</code></pre><p>Objective function: <span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span>, where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.HuberRegression" href="#MLJLinearModels.HuberRegression"><code>MLJLinearModels.HuberRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">HuberRegression()
HuberRegression(δ)
HuberRegression(δ, λ)
HuberRegression(δ, λ, γ; delta, lambda, gamma, penalty, fit_intercept, scale_penalty_with_samples, penalize_intercept)
</code></pre><p>Huber Regression with objective:</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>Where <code>ρ</code> is the Huber function <code>ρ(r) = r²/2</code><code>if</code>|r|≤δ<code>and</code>ρ(r)=δ(|r|-δ/2)<code>otherwise and</code><code>n</code><code>is the number of samples</code>size(X, 1)<code>. With</code>scale<em>penalty</em>with_samples = false<code>the objective function is</code><code>∑ρ(Xθ - y) + λ|θ|₂²/2 + γ|θ|₁</code>`.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.QuantileRegression" href="#MLJLinearModels.QuantileRegression"><code>MLJLinearModels.QuantileRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">QuantileRegression()
QuantileRegression(δ)
QuantileRegression(δ, λ)
QuantileRegression(δ, λ, γ; delta, lambda, gamma, penalty, fit_intercept, scale_penalty_with_samples, penalize_intercept)
</code></pre><p>Quantile Regression with objective:</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>Where <code>ρ</code> is the check function <code>ρ(r) = r(δ - 1(r &lt; 0))</code> and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L239">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LADRegression" href="#MLJLinearModels.LADRegression"><code>MLJLinearModels.LADRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LADRegression()
LADRegression(λ)
LADRegression(λ, γ; lambda, gamma, penalty, scale_penalty_with_samples, fit_intercept, penalize_intercept)
</code></pre><p>Least Absolute Deviation regression with objective:</p><p><span>$|Xθ - y|₁ + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₁ + λ|θ|₂²/2 + γ|θ|₁$</span>.</p><p>This is a specific type of Quantile Regression with <code>δ=0.5</code> (median).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L263">source</a></section></article><p><strong>Generic constructors</strong></p><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.GeneralizedLinearRegression" href="#MLJLinearModels.GeneralizedLinearRegression"><code>MLJLinearModels.GeneralizedLinearRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GeneralizedLinearRegression{L&lt;:Loss, P&lt;:Penalty}</code></pre><p>Generalized Linear Regression (GLR) model with objective function:</p><p><span>$L(y, Xθ) + P(θ)$</span></p><p>where <code>L</code> is a loss function, <code>P</code> a penalty, <code>y</code> is the vector of observed response, <code>X</code> is the feature matrix and <code>θ</code> the vector of parameters. If <code>scale_penalty_with_samples = true</code> (default) the penalty is automatically scaled with the number of samples.</p><p>Special cases include:</p><ul><li><strong>OLS regression</strong>:      L2 loss, no penalty.</li><li><strong>Ridge regression</strong>:    L2 loss, L2 penalty.</li><li><strong>Lasso regression</strong>:    L2 loss, L1 penalty.</li><li><strong>Logistic regression</strong>: Logit loss, [no,L1,L2] penalty.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L8-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RobustRegression" href="#MLJLinearModels.RobustRegression"><code>MLJLinearModels.RobustRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">RobustRegression()
RobustRegression(ρ)
RobustRegression(ρ, λ)
RobustRegression(ρ, λ, γ; rho, lambda, gamma, penalty, fit_intercept, scale_penalty_with_samples, penalize_intercept)
</code></pre><p>Objective function: <span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span> where ρ is a given function on the residuals and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L191">source</a></section></article><h3 id="Classification-1"><a class="docs-heading-anchor" href="#Classification-1">Classification</a><a class="docs-heading-anchor-permalink" href="#Classification-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LogisticRegression" href="#MLJLinearModels.LogisticRegression"><code>MLJLinearModels.LogisticRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">LogisticRegression()
LogisticRegression(λ)
LogisticRegression(λ, γ; lambda, gamma, penalty, fit_intercept, penalize_intercept, scale_penalty_with_samples, multi_class, nclasses)
</code></pre><p>Objective function: <span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <code>L</code> is either the logistic loss in the binary case or the multinomial loss otherwise and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.MultinomialRegression" href="#MLJLinearModels.MultinomialRegression"><code>MLJLinearModels.MultinomialRegression</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MultinomialRegression(a; kwa...)
</code></pre><p>Objective function: <span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span> where <code>L</code> is the multinomial loss and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/glr/constructors.jl#L176">source</a></section></article><h2 id="MLJ-Interface-1"><a class="docs-heading-anchor" href="#MLJ-Interface-1">MLJ Interface</a><a class="docs-heading-anchor-permalink" href="#MLJ-Interface-1" title="Permalink"></a></h2><h3 id="Regressors-1"><a class="docs-heading-anchor" href="#Regressors-1">Regressors</a><a class="docs-heading-anchor-permalink" href="#Regressors-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LinearRegressor" href="#MLJLinearModels.LinearRegressor"><code>MLJLinearModels.LinearRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Standard linear regression model with objective function</p><p><span>$|Xθ - y|₂²/2$</span></p><p><strong>Parameters</strong></p><ul><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use (if <code>nothing</code> the default is used). The solver is     Cholesky by default but can be Conjugate-Gradient as well. See <code>?Analytical</code>     for more information. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LinearRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L5-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RidgeRegressor" href="#MLJLinearModels.RidgeRegressor"><code>MLJLinearModels.RidgeRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Ridge regression model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2$</span></p><p>where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2$</span>.</p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L2 regularisation. Default: 1.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use (if <code>nothing</code> the default is used). The      solver is Cholesky by default but can be Conjugate-Gradient as      well. See <code>?Analytical</code> for more information. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(RidgeRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L33-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LassoRegressor" href="#MLJLinearModels.LassoRegressor"><code>MLJLinearModels.LassoRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Lasso regression model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₁$</span></p><p>where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₁$</span></p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L1 regularisation. Default: 1.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use (if <code>nothing</code> the default is used). Either <code>FISTA</code> or     <code>ISTA</code> can be used (proximal methods, with/without acceleration). Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LassoRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L75-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.ElasticNetRegressor" href="#MLJLinearModels.ElasticNetRegressor"><code>MLJLinearModels.ElasticNetRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Elastic net regression model with objective function</p><p><span>$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>where <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁$</span></p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the L2 regularisation. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regularisation. Default: 0.0</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use (if <code>nothing</code> the default is used). Either <code>FISTA</code> or     <code>ISTA</code> can be used (proximal methods, with/without acceleration). Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(ElasticNetRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L116-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.HuberRegressor" href="#MLJLinearModels.HuberRegressor"><code>MLJLinearModels.HuberRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Huber Regression is the same as <code>RobustRegressor</code> but with the robust loss set to <code>HuberRho</code>.</p><p><strong>Parameters</strong></p><ul><li><p><code>delta::Real</code></p><p>parametrises the <code>HuberRho</code> function (radius of the ball within which the loss is a quadratic loss) Default: 0.5</p></li><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regulariser if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(HuberRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L208-L217">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.QuantileRegressor" href="#MLJLinearModels.QuantileRegressor"><code>MLJLinearModels.QuantileRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Quantile Regression is the same as <code>RobustRegressor</code> but with the robust loss set to <code>QuantileRho</code>.</p><p><strong>Parameters</strong></p><ul><li><p><code>delta::Real</code></p><p>parametrises the <code>QuantileRho</code> function (indicating the  quantile to use with default <code>0.5</code> for the median regression) Default: 0.5</p></li><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regulariser if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(QuantileRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L252-L261">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LADRegressor" href="#MLJLinearModels.LADRegressor"><code>MLJLinearModels.LADRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Least Absolute Deviation regression with with objective function</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span></p><p>where <span>$ρ$</span> is the absolute loss and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span></p><p>See also <code>RobustRegressor</code>.</p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regulariser if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(LADRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L296-L314">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.RobustRegressor" href="#MLJLinearModels.RobustRegressor"><code>MLJLinearModels.RobustRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Robust regression model with objective function</p><p><span>$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$</span></p><p>where <span>$ρ$</span> is a robust loss function (e.g. the Huber function) and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$</span>.</p><p><strong>Parameters</strong></p><ul><li><p><code>rho::MLJLinearModels.RobustRho</code></p><p>the type of robust loss to use (see <code>HuberRho</code>, <code>TalwarRho</code>, ...) Default: HuberRho(0.1)</p></li><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regulariser if <code>penalty</code> is <code>:en</code>. Default: 1.0</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_regression()
mach = fit!(machine(RobustRegressor(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/regressors.jl#L159-L174">source</a></section></article><h3 id="Classifiers-1"><a class="docs-heading-anchor" href="#Classifiers-1">Classifiers</a><a class="docs-heading-anchor-permalink" href="#Classifiers-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.LogisticClassifier" href="#MLJLinearModels.LogisticClassifier"><code>MLJLinearModels.LogisticClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Logistic Classifier (typically called &quot;Logistic Regression&quot;). This model is a standard classifier for both binary and multiclass classification. In the binary case it corresponds to the LogisticLoss, in the multiclass to the Multinomial (softmax) loss. An elastic net penalty can be applied with overall objective function</p><p><span>$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$</span></p><p>where <span>$L$</span> is either the logistic or multinomial loss and <span>$λ$</span> and <span>$γ$</span> indicate the strength of the L2 (resp. L1) regularisation components and <span>$n$</span> is the number of samples <code>size(X, 1)</code>. With <code>scale_penalty_with_samples = false</code> the objective function is <span>$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$</span></p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code> and strength of the L2     regulariser if <code>penalty</code> is <code>:en</code>. Default: eps()</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_blobs(centers = 2)
mach = fit!(machine(LogisticClassifier(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/classifiers.jl#L5-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJLinearModels.MultinomialClassifier" href="#MLJLinearModels.MultinomialClassifier"><code>MLJLinearModels.MultinomialClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><p>See <code>LogisticClassifier</code>, it&#39;s the same except that multiple classes are assumed by default. The other parameters are the same.</p><p><strong>Parameters</strong></p><ul><li><p><code>lambda::Real</code></p><p>strength of the regulariser if <code>penalty</code> is <code>:l2</code> or <code>:l1</code>.     Strength of the L2 regulariser if <code>penalty</code> is <code>:en</code>. Default: eps()</p></li><li><p><code>gamma::Real</code></p><p>strength of the L1 regulariser if <code>penalty</code> is <code>:en</code>. Default: 0.0</p></li><li><p><code>penalty::Union{String, Symbol}</code></p><p>the penalty to use, either <code>:l2</code>, <code>:l1</code>, <code>:en</code> (elastic net) or <code>:none</code>. Default: :l2</p></li><li><p><code>fit_intercept::Bool</code></p><p>whether to fit the intercept or not. Default: true</p></li><li><p><code>penalize_intercept::Bool</code></p><p>whether to penalize the intercept. Default: false</p></li><li><p><code>scale_penalty_with_samples::Bool</code></p><p>whether to scale the penalty with the number of samples. Default: true</p></li><li><p><code>solver::Union{Nothing, MLJLinearModels.Solver}</code></p><p>type of solver to use, default if <code>nothing</code>. Default: nothing</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
X, y = make_blobs(centers = 3)
mach = fit!(machine(LogisticClassifier(), X, y))
predict(mach, X)
fitted_params(mach)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJLinearModels.jl/blob/76f6e48e42d133d2ab02551b060173b4e5d61f47/src/mlj/classifiers.jl#L59-L68">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../solvers/">« Solvers</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 15 November 2022 20:54">Tuesday 15 November 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
